{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cd37cf4",
   "metadata": {
    "id": "a461c72d"
   },
   "source": [
    "# OptiGuide Example\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc333c12",
   "metadata": {
    "id": "59a05fc7"
   },
   "source": [
    "Here we give a simple example, as designed and illustrated in the [OptiGuide paper](https://arxiv.org/abs/2307.03875).\n",
    "While the original paper is designed specifically for supply chain optimization, the general framework can be easily adapted to other applications with coding capacity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df57b42",
   "metadata": {
    "id": "5e92d200"
   },
   "source": [
    "## OptiGuide for Supply Chain Optimization: System Design Overview\n",
    "\n",
    "The original system design for OptiGuide, tailored for supply chain optimization, is presented below.\n",
    "\n",
    "The collaboration among three agents -- Coder, Safeguard, and Interpreter -- lies at the core of this system. They leverage a set of external tools and a large language model (LLM) to address users' questions related to supply chain applications. For a comprehensive understanding of the design and data flow, detailed information can be found in the original [paper](https://arxiv.org/abs/2307.03875).\n",
    "\n",
    "\n",
    "![optiguide system](https://www.beibinli.com/docs/optiguide/optiguide_system.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec618de",
   "metadata": {
    "id": "8b7f90c8"
   },
   "source": [
    "## New Implementation\n",
    "\n",
    "\n",
    "\n",
    "![](new_design.png)\n",
    "\n",
    "Advantages of this multi-agent design with autogen:\n",
    "- Collaborative Problem Solving: The collaboration among the user proxy agent and the assistant agents fosters a cooperative problem-solving environment. The agents can share information and knowledge, allowing them to complement each other's abilities and collectively arrive at better solutions. On the other hand, the Safeguard acts as a virtual adversarial checker, which can perform another safety check pass on the generated code.\n",
    "\n",
    "- Modularity: The division of tasks into separate agents promotes modularity in the system. Each agent can be developed, tested, and maintained independently, simplifying the overall development process and facilitating code management.\n",
    "\n",
    "- Memory Management: The OptiGuide agent's role in maintaining memory related to user interactions is crucial. The memory retention allows the agents to have context about a user's prior questions, making the decision-making process more informed and context-aware.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444fe547",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ReAgLnDma3oI",
    "outputId": "c95a23e8-ddb9-4fd2-f218-c40a51cbcbb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyautogen in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: diskcache in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from pyautogen) (5.6.3)\n",
      "Requirement already satisfied: flaml in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from pyautogen) (2.1.1)\n",
      "Requirement already satisfied: openai~=1.2 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from pyautogen) (1.3.6)\n",
      "Requirement already satisfied: python-dotenv in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from pyautogen) (0.21.0)\n",
      "Requirement already satisfied: termcolor in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from pyautogen) (2.3.0)\n",
      "Requirement already satisfied: tiktoken in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from pyautogen) (0.5.1)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from openai~=1.2->pyautogen) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from openai~=1.2->pyautogen) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from openai~=1.2->pyautogen) (0.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from openai~=1.2->pyautogen) (1.10.8)\n",
      "Requirement already satisfied: sniffio in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from openai~=1.2->pyautogen) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from openai~=1.2->pyautogen) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from openai~=1.2->pyautogen) (4.7.1)\n",
      "Requirement already satisfied: NumPy>=1.17.0rc1 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from flaml->pyautogen) (1.24.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from tiktoken->pyautogen) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from tiktoken->pyautogen) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from anyio<4,>=3.5.0->openai~=1.2->pyautogen) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai~=1.2->pyautogen) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai~=1.2->pyautogen) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.2->pyautogen) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken->pyautogen) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken->pyautogen) (1.26.16)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: autogen in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (1.0.16)\n",
      "Requirement already satisfied: PyYAML in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from autogen) (6.0)\n",
      "Requirement already satisfied: autopep8 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from autogen) (1.6.0)\n",
      "Requirement already satisfied: docopt in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from autogen) (0.6.2)\n",
      "Requirement already satisfied: setuptools in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from autogen) (68.0.0)\n",
      "Requirement already satisfied: twine in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from autogen) (4.0.2)\n",
      "Requirement already satisfied: pycodestyle>=2.8.0 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from autopep8->autogen) (2.10.0)\n",
      "Requirement already satisfied: toml in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from autopep8->autogen) (0.10.2)\n",
      "Requirement already satisfied: pkginfo>=1.8.1 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from twine->autogen) (1.9.6)\n",
      "Requirement already satisfied: readme-renderer>=35.0 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from twine->autogen) (42.0)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from twine->autogen) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt!=0.9.0,>=0.8.0 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from twine->autogen) (1.0.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from twine->autogen) (1.26.16)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from twine->autogen) (6.0.0)\n",
      "Requirement already satisfied: keyring>=15.1 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from twine->autogen) (23.13.1)\n",
      "Requirement already satisfied: rfc3986>=1.4.0 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from twine->autogen) (2.0.0)\n",
      "Requirement already satisfied: rich>=12.0.0 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from twine->autogen) (13.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from importlib-metadata>=3.6->twine->autogen) (3.11.0)\n",
      "Requirement already satisfied: jaraco.classes in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from keyring>=15.1->twine->autogen) (3.2.1)\n",
      "Requirement already satisfied: nh3>=0.2.14 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from readme-renderer>=35.0->twine->autogen) (0.2.14)\n",
      "Requirement already satisfied: docutils>=0.13.1 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from readme-renderer>=35.0->twine->autogen) (0.18.1)\n",
      "Requirement already satisfied: Pygments>=2.5.1 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from readme-renderer>=35.0->twine->autogen) (2.15.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from requests>=2.20->twine->autogen) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from requests>=2.20->twine->autogen) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from requests>=2.20->twine->autogen) (2023.7.22)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from rich>=12.0.0->twine->autogen) (2.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->twine->autogen) (0.1.0)\n",
      "Requirement already satisfied: more-itertools in /Users/coltonlapp/anaconda3/lib/python3.11/site-packages (from jaraco.classes->keyring>=15.1->twine->autogen) (8.12.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install Required Packages\n",
    "#%pip install optiguide\n",
    "# %pip install flaml[openai]\n",
    "%pip install pyautogen\n",
    "%pip install autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43cc7263",
   "metadata": {
    "id": "9a3b79c4"
   },
   "outputs": [],
   "source": [
    "# test Gurobi installation\n",
    "\n",
    "# %pip install autogen \n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from eventlet.timeout import Timeout\n",
    "\n",
    "# import auxillary packages\n",
    "import requests  # for loading the example source code\n",
    "import openai\n",
    "\n",
    "# import flaml and autogen\n",
    "#from flaml import autogen\n",
    "\n",
    "from flaml.autogen.agentchat import Agent, UserProxyAgent\n",
    "from flaml.autogen.agentchat import AssistantAgent\n",
    "from flaml.autogen.agentchat.agent import Agent\n",
    "from flaml.autogen.code_utils import extract_code\n",
    "from flaml import autogen \n",
    "\n",
    "from optiguide.optiguide import OptiGuideAgent\n",
    "\n",
    "# Colton trying to directly import the autogen package - doesn't work\n",
    "#from autogen.agentchat import AssistantAgent\n",
    "#from autogen.agentchat.agent import Agent\n",
    "#from autogen.code_utils import extract_code\n",
    "#from autogen import UserProxyAgent\n",
    "#from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "\n",
    "# %pip install flaml[openai]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08c1e211",
   "metadata": {
    "id": "aedf19e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#autogen.oai.ChatCompletion.start_logging()\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST.json\",\n",
    "    file_location=\".\",\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            #\"gpt-4\",\n",
    "            #\"gpt4\",\n",
    "            #\"gpt-4-32k\",\n",
    "            #\"gpt-4-32k-0314\",\n",
    "            #\"gpt-3.5-turbo\"\n",
    "            \"gpt-3.5-turbo-16k\"\n",
    "            #\"gpt-3.5-turbo-0301\",\n",
    "            #\"chatgpt-35-turbo-0301\",\n",
    "            #\"gpt-35-turbo-v0301\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "config_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2944272d",
   "metadata": {
    "id": "e9e7e728"
   },
   "source": [
    "Now, let's import the source code (loading from URL) and also some training examples (defined as string blow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05664eac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca962ac5",
    "outputId": "4a789991-8ba1-46f3-aad5-7fbaaaff7f02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import re\n",
      "import time\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import gurobipy as gp\n",
      "import sys\n",
      "from gurobipy import GRB\n",
      "from datetime import datetime\n",
      "from selenium import webdriver\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      "                    print(f\"Sell to Market {i + 1}: {sell[p, i].x}\")\n",
      "\n",
      "            print(f\"\\nTotal Profit: {model.objVal}\")\n",
      "        else:\n",
      "            print(\"No solution found\")\n",
      "\n",
      "    return [model, buy, sell]\n",
      "\n",
      "\n",
      "run(parameters, print_results = True )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'import os\\nimport re\\nimport time\\nimport numpy as np\\nimport pandas as pd\\nimport gurobipy as gp\\nimport sys\\nfrom gurobipy import GRB\\nfrom datetime import datetime\\nfrom selenium import webdriver\\nfrom webdriver_manager.chrome import ChromeDriverManager\\n\\nfrom web_scrape_price_data import extract_date, download_price_data, extract_time_series_prices\\nfrom make_plots import plot_price_time_series\\n\\n# Set parameters for the model\\n\\nparameters = {\\n    \\'name\\': \\'ElectricityArbitrage\\',\\n    \\'generator_name\\': \\'ADK HUDSON___FALLS\\',\\n    \\'start_date\\': None,\\n    \\'num_periods\\': 24,\\n    \\'num_markets\\': 1,\\n    \\'num_batteries\\': 8,\\n    \\'battery_capacity\\': 100,\\n    \\'charge_loss\\': 0.95,\\n    \\'max_charge\\': 50,\\n    \\'max_discharge\\': 100\\n}\\n\\n# OPTIGUIDE DATA CODE GOES HERE\\n\\n\\ndef create_model(parameters): # num_markets is going to be 1 for the time being\\n    name = parameters[\\'name\\']\\n    generator_name = parameters[\\'generator_name\\']\\n    start_date = parameters[\\'start_date\\']\\n    num_periods = parameters[\\'num_periods\\']\\n    num_markets = parameters[\\'num_markets\\']\\n    num_batteries = parameters[\\'num_batteries\\']\\n    battery_capacity = parameters[\\'battery_capacity\\']\\n    charge_loss = parameters[\\'charge_loss\\']\\n    max_charge = parameters[\\'max_charge\\']\\n    max_discharge = parameters[\\'max_discharge\\']\\n\\n    model = gp.Model(name)\\n\\n    periods = range(num_periods)\\n    markets = range(num_markets)\\n\\n    buy = model.addVars(num_periods, num_markets, vtype=GRB.CONTINUOUS, name=\\'Buy\\')\\n    sell = model.addVars(num_periods, num_markets, vtype=GRB.CONTINUOUS, name=\\'Sell\\')\\n\\n\\n    # placeholder\\n    if start_date is None:\\n        start_date = datetime.today()\\n        start_date = start_date.strftime(\"%Y%m%d\")\\n\\n    # Download data\\n    download_price_data(start_date, generator_name)\\n\\n    # Plot prices time series\\n    plot_price_time_series( start_date, generator_name)\\n\\n    # Extract time series of prices in a list for Gurobi\\n    prices_dict = extract_time_series_prices( start_date, generator_name)\\n    price_times = prices_dict[\\'times\\']\\n    prices = prices_dict[\\'prices\\']\\n\\n\\n\\n\\n\\n    #model.setObjective(\\n    #    gp.quicksum(prices[p][i] * sell[p, i] - prices[p][i] * buy[p, i] for p in periods for i in markets),\\n    #    GRB.MAXIMIZE\\n    #)\\n\\n    model.setObjective(\\n        gp.quicksum(prices[p] * sell[p, i] - prices[p] * buy[p, i] for p in periods for i in markets),\\n        GRB.MAXIMIZE\\n    )\\n\\n    for p in range(num_periods):\\n        current_level = np.sum(charge_loss * buy[p_, i] - sell[p_, i] for p_ in range(p) for i in markets)\\n\\n        # model.addConstr(current_level <= 0, f\\'EnoughToSellConstraint_period_{p+1}\\')\\n\\n        for i in markets:\\n            model.addConstr(current_level <= battery_capacity * num_batteries, f\\'CapacityConstraint_period_{p+1}\\')\\n            model.addConstr(sell[p, i] <= current_level, f\\'SupplyConstraint_period_{p+1}\\')\\n            model.addConstr(buy[p, i] * charge_loss <= max_charge, f\\'ChargeConstraint_period_{p+1}\\')\\n            model.addConstr(sell[p, i] <= max_discharge, f\\'DischargeConstraint_period_{p+1}\\')\\n\\n    # OPTIGUIDE CONSTRAINT CODE GOES HERE\\n\\n    return [model, buy, sell]\\n\\n\\n# Run the model \\ndef run(parameters, print_results = False ):\\n\\n    # Create model\\n    [model, buy, sell] = create_model(parameters)\\n\\n    # Run model\\n    model.optimize()\\n\\n    if print_results:\\n        num_periods = parameters[\\'num_periods\\']\\n        num_markets = parameters[\\'num_markets\\']\\n\\n        if model.status == GRB.OPTIMAL:\\n            print(\"\\\\nOptimal Solution:\")\\n\\n            for p in range(num_periods):\\n                print(f\"\\\\nPeriod {p + 1}:\")\\n\\n                for i in range(num_markets):\\n                    print(f\"Buy from Market {i + 1}: {buy[p, i].x}\")\\n                    print(f\"Sell to Market {i + 1}: {sell[p, i].x}\")\\n\\n            print(f\"\\\\nTotal Profit: {model.objVal}\")\\n        else:\\n            print(\"No solution found\")\\n\\n    return [model, buy, sell]\\n\\n\\nrun(parameters, print_results = True )'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_type = 'py_file' #options: demo, py_file, notebook\n",
    "\n",
    "if code_type == 'demo':\n",
    "    # Get the source code of our coffee example\n",
    "    code_url = \"https://raw.githubusercontent.com/microsoft/OptiGuide/main/benchmark/application/coffee.py\"\n",
    "    response  = requests.get(code_url)\n",
    "\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Get the text content from the response\n",
    "        code = response.text\n",
    "    else:\n",
    "        raise RuntimeError(\"Failed to retrieve the file.\")\n",
    "elif code_type == 'py_file':\n",
    "    code = open( 'run_model.py', \"r\").read() # for local files\n",
    "\n",
    "elif code_type == 'notebook':\n",
    "    # Extract code from jupyter notebook\n",
    "    import nbformat\n",
    "    def extract_code_from_notebook(notebook_path):\n",
    "        try:\n",
    "            with open(notebook_path, 'r', encoding='utf-8') as notebook_file:\n",
    "                notebook_content = nbformat.read(notebook_file, as_version=4)\n",
    "\n",
    "            code_cells = []\n",
    "            for cell in notebook_content['cells']:\n",
    "                if cell['cell_type'] == 'code':\n",
    "                    code_cells.append(cell['source'])\n",
    "\n",
    "            code_as_string = '\\n'.join(code_cells)\n",
    "            return code_as_string\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Notebook file '{notebook_path}' not found.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading notebook: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Example usage\n",
    "    notebook_path = 'Battery_Optimization_Shen_Lapp_Poser.ipynb'\n",
    "    code = extract_code_from_notebook(notebook_path)\n",
    "\n",
    "\n",
    "# show the first head and tail of the source code\n",
    "print(\"\\n\".join(code.split(\"\\n\")[:10]))\n",
    "print(\".\\n\" * 3)\n",
    "print(\"\\n\".join(code.split(\"\\n\")[-10:]))\n",
    "\n",
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a70331b",
   "metadata": {
    "code_folding": [],
    "id": "e31c4b36"
   },
   "outputs": [],
   "source": [
    "# In-context learning examples.\n",
    "example_qa = \"\"\"\n",
    "----------\n",
    "Question: What if the battery charge capacity was only 70 percent instead of 95 percent?\n",
    "Answer Code:\n",
    "```python\n",
    "parameters['charge_loss'] = .7\n",
    "```\n",
    "----------\n",
    "Question: What if we had 20 batteries instead of 8?\n",
    "Answer Code:\n",
    "```python\n",
    " parameters['num_batteries'] = 8\n",
    "```\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code.find( '# OPTIGUIDE DATA CODE GOES HERE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8364c898",
   "metadata": {
    "id": "5a5a7d7e"
   },
   "source": [
    "Now, let's create an OptiGuide agent and also a user.\n",
    "\n",
    "For the OptiGuide agent, we only allow \"debug_times\" to be 1, which means it can debug its answer once if it encountered errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4346027a",
   "metadata": {
    "id": "af53727c"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "agent = OptiGuideAgent(\n",
    "    name=\"OptiGuide Super sexy team Example\",\n",
    "    source_code=code,\n",
    "    debug_times=1,\n",
    "    example_qa=\"\",\n",
    "    llm_config={\n",
    "        \"request_timeout\": 600,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "    }\n",
    ")\n",
    "\n",
    "user = UserProxyAgent(\n",
    "    \"user\", max_consecutive_auto_reply=0,\n",
    "    human_input_mode=\"NEVER\", code_execution_config=False\n",
    ")\n",
    "\n",
    "# Get open ai key from txt file\n",
    "api_path = '../credentials/openai_api_key.txt'\n",
    "\n",
    "def read_api_key(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            api_key = file.read().strip()\n",
    "        return api_key\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        return None\n",
    "    \n",
    "openai.api_key = read_api_key(api_path)\n",
    "openai.api_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d3c84",
   "metadata": {
    "id": "bd615e87"
   },
   "source": [
    "Now, let's create a user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95f7aef0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24a76f67",
    "outputId": "2399e32e-fe3f-429f-9c40-fcc75605741d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to OptiGuide Super sexy team Example):\n",
      "\n",
      "What would the profit be if we increased the max charge to 70?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mOptiGuide Super sexy team Example\u001b[0m (to writer):\n",
      "\n",
      "\n",
      "Answer Code:\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "please install flaml[openai] option to use the flaml.autogen.oai subpackage.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/coltonlapp/Dropbox/My Mac (Coltons-MacBook-Pro.local)/Desktop/SCHOOL/Year2_FALL/DABP/final_project/EnergyArbitrage/Code/optiguide_example.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/coltonlapp/Dropbox/My%20Mac%20%28Coltons-MacBook-Pro.local%29/Desktop/SCHOOL/Year2_FALL/DABP/final_project/EnergyArbitrage/Code/optiguide_example.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m user\u001b[39m.\u001b[39minitiate_chat(agent, message\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWhat would the profit be if we increased the max charge to 70?\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/flaml/autogen/agentchat/conversable_agent.py:521\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \n\u001b[1;32m    509\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 521\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_init_message(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcontext), recipient, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/flaml/autogen/agentchat/conversable_agent.py:324\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    322\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    323\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 324\u001b[0m     recipient\u001b[39m.\u001b[39mreceive(message, \u001b[39mself\u001b[39m, request_reply, silent)\n\u001b[1;32m    325\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    328\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/flaml/autogen/agentchat/conversable_agent.py:452\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 452\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_reply(messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_messages[sender], sender\u001b[39m=\u001b[39msender)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optiguide/optiguide.py:136\u001b[0m, in \u001b[0;36mOptiGuideAgent.generate_reply\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_success \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m# Step 2-6: code, safeguard, and interpret\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitiate_chat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_writer, message\u001b[39m=\u001b[39mCODE_PROMPT)\n\u001b[1;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_success:\n\u001b[1;32m    138\u001b[0m     \u001b[39m# step 7: receive interpret result\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_message(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_writer)[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/flaml/autogen/agentchat/conversable_agent.py:521\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \n\u001b[1;32m    509\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 521\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_init_message(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcontext), recipient, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/flaml/autogen/agentchat/conversable_agent.py:324\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    322\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    323\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 324\u001b[0m     recipient\u001b[39m.\u001b[39mreceive(message, \u001b[39mself\u001b[39m, request_reply, silent)\n\u001b[1;32m    325\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    328\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/flaml/autogen/agentchat/conversable_agent.py:452\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 452\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_reply(messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_messages[sender], sender\u001b[39m=\u001b[39msender)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/flaml/autogen/agentchat/conversable_agent.py:764\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[0;32m--> 764\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39m, messages\u001b[39m=\u001b[39mmessages, sender\u001b[39m=\u001b[39msender, config\u001b[39m=\u001b[39mreply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    765\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[1;32m    766\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/flaml/autogen/agentchat/conversable_agent.py:596\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    593\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m    595\u001b[0m \u001b[39m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m--> 596\u001b[0m response \u001b[39m=\u001b[39m oai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mcreate(\n\u001b[1;32m    597\u001b[0m     context\u001b[39m=\u001b[39mmessages[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m), messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_system_message \u001b[39m+\u001b[39m messages, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mllm_config\n\u001b[1;32m    598\u001b[0m )\n\u001b[1;32m    599\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, oai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mextract_text_or_function_call(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/flaml/autogen/oai/completion.py:758\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Make a completion for a given context.\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \n\u001b[1;32m    694\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[39m        - `pass_filter`: whether the response passes the filter function. None if no filter is provided.\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \u001b[39mif\u001b[39;00m ERROR:\n\u001b[0;32m--> 758\u001b[0m     \u001b[39mraise\u001b[39;00m ERROR\n\u001b[1;32m    759\u001b[0m \u001b[39mif\u001b[39;00m config_list:\n\u001b[1;32m    760\u001b[0m     last \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(config_list) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: please install flaml[openai] option to use the flaml.autogen.oai subpackage."
     ]
    }
   ],
   "source": [
    "user.initiate_chat(agent, message='What would the profit be if we increased the max charge to 70?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "726edebc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbdd1f28",
    "outputId": "c5e943cf-fce7-4484-8cd8-433d4fede8e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to OptiGuide Coffee Example):\n",
      "\n",
      "What is the impact of supplier1 being able to supply only half the quantity at present?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mOptiGuide Coffee Example\u001b[0m (to writer):\n",
      "\n",
      "\n",
      "Answer Code:\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mwriter\u001b[0m (to OptiGuide Coffee Example):\n",
      "\n",
      "```python\n",
      "# Adjust the supplier capacity\n",
      "capacity_in_supplier['supplier1'] /= 2\n",
      "\n",
      "# Update supply constraints\n",
      "for s in set(i[0] for i in shipping_cost_from_supplier_to_roastery.keys()):\n",
      "    if s == 'supplier1':\n",
      "        model.remove(model.getConstrByName(f\"supply_{s}\"))\n",
      "        model.addConstr(sum(x[i] for i in shipping_cost_from_supplier_to_roastery.keys() if i[0] == s) <= capacity_in_supplier[s], f\"supply_{s}\")\n",
      "# Rerun the optimization\n",
      "model.optimize()\n",
      "\n",
      "if model.status == GRB.OPTIMAL:\n",
      "    print(f'Optimal cost after supplier1 supply reduction: {model.objVal}')\n",
      "else:\n",
      "    print(\"Not solved to optimality. Optimization status:\", model.status)\n",
      "```\n",
      "\n",
      "Explanation:\n",
      "First, we reduce the capacity of 'supplier1' by half in the 'capacity_in_supplier' dictionary. Then, we update 'supplier1' constraint in the model: we first remove its existing supply constraint, and then add a new one with the updated capacity. After these updates, we run the optimization again. If the solution is optimal, we print out the new optimal cost. If not, we print out the optimization status.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mOptiGuide Coffee Example\u001b[0m (to safeguard):\n",
      "\n",
      "\n",
      "--- Code ---\n",
      "# Adjust the supplier capacity\n",
      "capacity_in_supplier['supplier1'] /= 2\n",
      "\n",
      "# Update supply constraints\n",
      "for s in set(i[0] for i in shipping_cost_from_supplier_to_roastery.keys()):\n",
      "    if s == 'supplier1':\n",
      "        model.remove(model.getConstrByName(f\"supply_{s}\"))\n",
      "        model.addConstr(sum(x[i] for i in shipping_cost_from_supplier_to_roastery.keys() if i[0] == s) <= capacity_in_supplier[s], f\"supply_{s}\")\n",
      "# Rerun the optimization\n",
      "model.optimize()\n",
      "\n",
      "if model.status == GRB.OPTIMAL:\n",
      "    print(f'Optimal cost after supplier1 supply reduction: {model.objVal}')\n",
      "else:\n",
      "    print(\"Not solved to optimality. Optimization status:\", model.status)\n",
      "\n",
      "--- One-Word Answer: SAFE or DANGER ---\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msafeguard\u001b[0m (to OptiGuide Coffee Example):\n",
      "\n",
      "SAFE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (mac64[rosetta2])\n",
      "\n",
      "CPU model: Apple M1 Pro\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 11 rows, 18 columns and 36 nonzeros\n",
      "Model fingerprint: 0x01a67d7b\n",
      "Variable types: 0 continuous, 18 integer (0 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [2e+00, 1e+01]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+01, 2e+02]\n",
      "Found heuristic solution: objective 3280.0000000\n",
      "Presolve time: 0.00s\n",
      "Presolved: 11 rows, 18 columns, 36 nonzeros\n",
      "Variable types: 0 continuous, 18 integer (0 binary)\n",
      "Found heuristic solution: objective 3276.0000000\n",
      "\n",
      "Root relaxation: objective 2.470000e+03, 11 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0    2470.0000000 2470.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (11 simplex iterations) in 0.01 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 3: 2470 3276 3280 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 2.470000000000e+03, best bound 2.470000000000e+03, gap 0.0000%\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (mac64[rosetta2])\n",
      "\n",
      "CPU model: Apple M1 Pro\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 11 rows, 18 columns and 36 nonzeros\n",
      "Model fingerprint: 0x591c7dc1\n",
      "Variable types: 0 continuous, 18 integer (0 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [2e+00, 1e+01]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+01, 1e+02]\n",
      "\n",
      "MIP start from previous solve did not produce a new incumbent solution\n",
      "MIP start from previous solve violates constraint supply_supplier1 by 5.000000000\n",
      "\n",
      "Presolve time: 0.00s\n",
      "Presolved: 11 rows, 18 columns, 36 nonzeros\n",
      "Variable types: 0 continuous, 18 integer (0 binary)\n",
      "\n",
      "Root relaxation: infeasible, 14 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 infeasible    0               - infeasible      -     -    0s\n",
      "\n",
      "Explored 1 nodes (14 simplex iterations) in 0.01 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 0\n",
      "\n",
      "Model is infeasible\n",
      "Best objective -, best bound -, gap -\n",
      "Not solved to optimality. Optimization status: 3\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (mac64[rosetta2])\n",
      "\n",
      "CPU model: Apple M1 Pro\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 11 rows, 18 columns and 36 nonzeros\n",
      "Model fingerprint: 0x591c7dc1\n",
      "Variable types: 0 continuous, 18 integer (0 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [2e+00, 1e+01]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+01, 1e+02]\n",
      "Presolved: 11 rows, 18 columns, 36 nonzeros\n",
      "\n",
      "Continuing optimization...\n",
      "\n",
      "\n",
      "Explored 1 nodes (14 simplex iterations) in 0.00 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 0\n",
      "\n",
      "Model is infeasible\n",
      "Best objective -, best bound -, gap -\n",
      "Tue Nov 14 13:10:20 2023\n",
      "Not solved to optimality. Optimization status: 3\n",
      "\n",
      "Computing Irreducible Inconsistent Subsystem (IIS)...\n",
      "\n",
      "           Constraints          |            Bounds           |  Runtime\n",
      "      Min       Max     Guess   |   Min       Max     Guess   |\n",
      "--------------------------------------------------------------------------\n",
      "        0        11         -         0        18         -           0s\n",
      "       11        11        11         0         0         0           0s\n",
      "\n",
      "IIS computed: 11 constraints, 0 bounds\n",
      "IIS runtime: 0.01 seconds (0.00 work units)\n",
      "\u001b[33minfeasible\n",
      "Conflicting Constraints:\n",
      "['flow_roastery1', 'flow_roastery2', 'supply_supplier2', 'supply_supplier3', 'light_demand_cafe2', 'dark_demand_cafe2', 'light_demand_cafe3', 'dark_demand_cafe3', 'light_demand_cafe1', 'dark_demand_cafe1', 'supply_supplier1']\u001b[0m\n",
      "\u001b[33mOptiGuide Coffee Example\u001b[0m (to writer):\n",
      "\n",
      "Here are the execution results: infeasible\n",
      "Conflicting Constraints:\n",
      "['flow_roastery1', 'flow_roastery2', 'supply_supplier2', 'supply_supplier3', 'light_demand_cafe2', 'dark_demand_cafe2', 'light_demand_cafe3', 'dark_demand_cafe3', 'light_demand_cafe1', 'dark_demand_cafe1', 'supply_supplier1']\n",
      "\n",
      "Can you organize these information to a human readable answer?\n",
      "Remember to compare the new results to the original results you obtained in the\n",
      "beginning.\n",
      "\n",
      "--- HUMAN READABLE ANSWER ---\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mwriter\u001b[0m (to OptiGuide Coffee Example):\n",
      "\n",
      "After reducing the supply capacity of 'supplier1' by half, the supply chain optimization problem became infeasible. This means that with the current capacities of suppliers, roasteries, and shipper, we were not able to meet all demands from all cafes.\n",
      "\n",
      "The list of conflicting constraints is:\n",
      "* Flows from roasteries 'roastery1' and 'roastery2'\n",
      "* The supply from 'supplier2', 'supplier3', and 'supplier1'\n",
      "* Light and dark coffee demand for 'cafe1', 'cafe2', and 'cafe3'\n",
      "\n",
      "This indicates issues with ensuring supply, distributing supply to roasteries, and meeting coffee demands.\n",
      "\n",
      "To address this, we may need to increase the capacities of other suppliers, increase the throughput of roasteries, or amend the distribution strategy. Alternatively, it may be necessary to negotiate the demands with cafes, in order to match the available supplies.\n",
      "\n",
      "Remember, the original cost before capacity change of 'supplier1' was 2470, which is not achievable under the current situation. The optimization result indicates critical dependency on 'supplier1' and noises the need for diversifying the sources or increasing flexibilities in supply chain design.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mOptiGuide Coffee Example\u001b[0m (to user):\n",
      "\n",
      "After reducing the supply capacity of 'supplier1' by half, the supply chain optimization problem became infeasible. This means that with the current capacities of suppliers, roasteries, and shipper, we were not able to meet all demands from all cafes.\n",
      "\n",
      "The list of conflicting constraints is:\n",
      "* Flows from roasteries 'roastery1' and 'roastery2'\n",
      "* The supply from 'supplier2', 'supplier3', and 'supplier1'\n",
      "* Light and dark coffee demand for 'cafe1', 'cafe2', and 'cafe3'\n",
      "\n",
      "This indicates issues with ensuring supply, distributing supply to roasteries, and meeting coffee demands.\n",
      "\n",
      "To address this, we may need to increase the capacities of other suppliers, increase the throughput of roasteries, or amend the distribution strategy. Alternatively, it may be necessary to negotiate the demands with cafes, in order to match the available supplies.\n",
      "\n",
      "Remember, the original cost before capacity change of 'supplier1' was 2470, which is not achievable under the current situation. The optimization result indicates critical dependency on 'supplier1' and noises the need for diversifying the sources or increasing flexibilities in supply chain design.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "user.initiate_chat(agent, message=\"What is the impact of supplier1 being able to supply only half the quantity at present?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb71ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1668b480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
